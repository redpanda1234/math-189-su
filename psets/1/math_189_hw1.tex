\documentclass[189]{pset}

% ================================================================== %
%                                                                    %
%                              Document                              %
%                                                                    %
% ================================================================== %

% ----------------------- Header formatting ------------------------ %

\name{Forest Kobayashi}
\class{Math of Big Data}
\prof{Gu}
\assignment{1}
\duedate{05/15/2018}
\dueday{Tuesday}
\problems{1, 2}
\acknowledgements{{}, {}}
\onTime{0}

\comments{Feel free to work with other students, but make sure you
  write up the homework and code on your own (no copying homework
  \textit{or} code; no pair programming). Feel free to ask students or
  instructors for help debugging code or whatever else, though.

  The starter code for problem 2 part c and d can be found under the
  Resource tab on course website.

  \textit{Note:} You need to create a Github account for submission of
  the coding part of the homework. Please create a repository on
  Github to hold all your code and include your Github account
  username as part of the answer to problem 2.}

\lfoot{Due Tuesday, May 15th 2018}

\begin{document}

% --------------------------- Problem 1 ---------------------------- %

  \section{(Linear Transformation)}
    Let $\vy = A \vx + \vb$ be a random vector. Show that
    expectation is linear:
    \[
      \EE[\vy] = \EE[A\vx + \vb] = A\EE[\vx] + \vb.
    \]
    Also show that
    \[
      \cov[\vy] = \cov[A\vx + \vb] = A \cov[\vx] A^\T = A\mb\Sigma A^\T.
    \]

  \hrulefill

  \section*{Solution}

  \clearpage

% --------------------------- Problem 2 ---------------------------- %

  \section{}
    Given the dataset $\cD = \{(x,y)\} = \{(0,1), (2,3), (3,6), (4,8)\}$
    \begin{enumerate}
      \item Find the least squares estimate $y = {\bm{\theta}}^\T\vx$ by
        hand using Cramer's Rule.
      \item Use the normal equations to find the same solution and
        verify it is the same as part (a).
      \item Plot the data and the optimal linear fit you found.
      \item Find randomly generate 100 points near the line with white
        Gaussian noise and then compute the least squares estimate
        (using a computer). Verify that this new line is close to the
        original and plot the new dataset, the old line, and the new
        line.
    \end{enumerate}

  \hrulefill

  \section*{Solution}


\end{document}
