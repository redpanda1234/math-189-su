\documentclass[189]{pset}

% ================================================================== %
%                                                                    %
%                              Document                              %
%                                                                    %
% ================================================================== %

% ----------------------- Header formatting ------------------------ %

\name{Forest Kobayashi}
\class{Math of Big Data}
\season{Summer}
\prof{Gu}
\assignment{1}
\duedate{05/15/2018}
\dueday{Tuesday}
\problems{1, 2}
\acknowledgements{{Tim Player, Jacky Lee}, {}}
\onTime{0}

\comments{\textbf{Comments}: Feel free to work with other students,
  but make sure you write up the homework and code on your own (no
  copying homework \textit{or} code; no pair programming). Feel free
  to ask students or instructors for help debugging code or whatever
  else, though.

  The starter code for problem 2 part c and d can be found under the
  Resource tab on course website.

  \textit{Note:} You need to create a Github account for submission of
  the coding part of the homework. Please create a repository on
  Github to hold all your code and include your Github account
  username as part of the answer to problem 2.}

\lfoot{Due Tuesday, May 15th 2018}

\begin{document}

% --------------------------- Problem 1 ---------------------------- %

  \section{(Linear Transformation)}
    Let $\vy = A \vx + \vb$ be a random vector. Show that
    expectation is linear:
    \[
      \EE[\vy] = \EE[A\vx + \vb] = A\EE[\vx] + \vb.
    \]
    Also show that
    \[
      \cov[\vy] = \cov[A\vx + \vb] = A \cov[\vx] A^\T = A\mb\Sigma A^\T.
    \]

  \hrulefill

  \section*{Solution:}
    \begin{enumerate}
      \item First, we prove some small lemmas.
        \begin{lemma}
          Let $X$ be a continuous random variable, and $a$ be a
          scalar. Then $\EE\bk{aX}=a\EE\bk{X}$.
        \end{lemma}
        \begin{proof}
          Let $X$ admit a density function $f(x)$. Then
          \begin{align*}
            \EE\bk{aX}
            &= \int_{-\infty}^\infty axf(x) \dd x \\
            &= a\int_{-\infty}^\infty xf(x) \dd x \\
            &= a\EE\bk{X}
          \end{align*}
        \end{proof}
        \begin{lemma}
          Let $X$ be a random variable, and let $a$ be a scalar. Then
          $\EE\bk{X+a}=\EE\bk{X}+\EE\bk{a}=\EE\bk{X}+a$.
        \end{lemma}
        \begin{proof}
          Let $X$ admit a density function $f(x)$. Then over $\RR$,
          $f(x)$ must integrate to 1. Hence
          \begin{align*}
            \int_{-\infty}^\infty (x+a)f(x) \dd x
            &= \int_{-\infty}^\infty xf(x) + af(x) \dd x \\
            &= \int_{-\infty}^\infty xf(x) \dd x +
              \int_{-\infty}^\infty af(x) \dd x \\
            &= \int_{-\infty}^\infty xf(x) \dd x +
              a\int_{-\infty}^\infty f(x) \dd x \\
            &= \EE\bk{X} + a\cdot 1 \\
            &= \EE\bk{X} + a
          \end{align*}
        \end{proof}
        \begin{lemma}
          Let $X$ and $Y$ be continuous random variables. Then
          $\EE\bk{X+Y} = \EE\bk{X}+\EE\bk{Y}$.
        \end{lemma}
        \begin{proof}
          Let $X+Y$ have a density function $f(x,y)$. Then
          \begin{align*}
            \EE\bk{X+Y}
            &= \int_{-\infty}^\infty \int_{-\infty}^\infty
              (x+y)f(x,y)\dd x \dd y \\
            &= \int_{-\infty}^\infty
          \end{align*}
        \end{proof}
        \begin{align*}
          \EE\bk{\vy}
          &= \EE\bk{A\vx + \vb} \\
          &= \EE\bk{A\vx} + \EE\bk{\vb} \\
          &= \EE\bk{A\vx} + \vb
        \end{align*}
        (we got from the second line to the third by the fact that
        $\EE\bk{X + Y} = \EE\bk{X} + \EE\bk{Y}$ for random variables
        $X$ and $Y$, and from the second to the third by the fact that
        the expected value of a constant is the constant itself). It
        remains to show that $\EE\bk{A\vx} = A\EE\bk{\vx}$. Suppose
        $\vx$ is an $n$-dimensional random vector in a space $X$:
        \[
          \vx =
          \begin{bmatrix}
            x_0 \\
            x_1 \\
            \vdots \\
            x_n
          \end{bmatrix}
        \]
        where each of the $x_i \in \RR$. By definition, the
        expectation value of $\vx$ is given by
        \[
          \EE\bk{\vx} =
          \begin{bmatrix}
            \EE\bk{x_0} \\
            \EE\bk{x_1} \\
            \vdots \\
            \EE\bk{x_n}
          \end{bmatrix}
        \]
        and for any continuous random variable, we have
        \[
          \EE\bk{Y} = \int_{-\infty}^\infty y f(y) \dd y
        \]
        where $f(y)$ is some probability density function. For each of
        the $x_i$, define $f_i(x_i)$ to be the corresponding
        probability density function. Then
        \begin{align*}
          \EE\bk{\vx}
          &=
            \begin{bmatrix}
              \displaystyle\int_{-\infty}^\infty x_0 f_0(x_0) \dd x_0
              \\[1em]
              \displaystyle\int_{-\infty}^\infty x_1 f_1(x_1) \dd x_1
              \\[1em]
              \vdots \\[1em]
              \displaystyle\int_{-\infty}^\infty x_n f_n(x_n) \dd x_n
            \end{bmatrix}
        \end{align*}
        Let $A \in M_{m\times n}(\RR)$, with rows $\va_0$, $\va_1$,
        $\ldots$, $\va_m$. Then $A\EE\bk{\vx}$ is given by
        \begin{align*}
          A\EE\bk{\vx}
          &=
            \begin{bmatrix}
              a_{0,0} & a_{0,1} & \cdots & a_{0,n} \\
              a_{1,0} & a_{1,1} & \cdots & a_{1,n} \\
              \vdots & \vdots & \ddots & \vdots \\
              a_{m,0} & a_{m,1} & \cdots & a_{m,n}
            \end{bmatrix}
            \begin{bmatrix}
              \EE\bk{x_0} \\
              \EE\bk{x_1} \\
              \vdots \\
              \EE\bk{x_n}
            \end{bmatrix} \\
          &=
            \begin{bmatrix}
              \displaystyle\sum_{i=0}^n a_{0,i}\EE\bk{x_i} \\[1em]
              \displaystyle\sum_{i=0}^n a_{1,i}\EE\bk{x_i} \\[1em]
              \vdots\\
              \displaystyle\sum_{i=0}^n a_{n,i}\EE\bk{x_i}
            \end{bmatrix}
        \end{align*}
        Now, we manipulate this expression to obtain $\EE\bk{A\vx}$.
        Because the expected value of a sum is the sum of expected
        values, we have
        \begin{align*}
          \begin{bmatrix}
            \displaystyle\sum_{i=0}^n a_{0,i}\EE\bk{x_i} \\[1em]
            \displaystyle\sum_{i=0}^n a_{1,i}\EE\bk{x_i} \\[1em]
            \vdots\\
            \displaystyle\sum_{i=0}^n a_{n,i}\EE\bk{x_i}
          \end{bmatrix}
          &=
            \begin{bmatrix}
              \displaystyle\EE\bk{\sum_{i=0}^n a_{0,i}x_i} \\[1.5em]
              \displaystyle\EE\bk{\sum_{i=0}^n a_{1,i}x_i} \\[1.5em]
              \vdots\\
              \displaystyle\EE\bk{\sum_{i=0}^n a_{n,i}x_i}
            \end{bmatrix}
          \shortintertext{and now because the expected value of a
          vector is a vector of expected values,}
          \begin{bmatrix}
            \displaystyle\EE\bk{\sum_{i=0}^n a_{0,i}x_i} \\[1.5em]
            \displaystyle\EE\bk{\sum_{i=0}^n a_{1,i}x_i} \\[1.5em]
            \vdots\\
            \displaystyle\EE\bk{\sum_{i=0}^n a_{n,i}x_i}
          \end{bmatrix}
          &= \EE\bk{
            \begin{bmatrix}
              \displaystyle \sum_{i=0}^n a_{0,i} x_i \\[1em]
              \displaystyle \sum_{i=0}^n a_{1,i} x_i \\[1em]
              \vdots \\
              \displaystyle \sum_{i=0}^n a_{m,i} x_i
            \end{bmatrix}
          }
        \end{align*}
      \item The covariance matrix $\cov\bk{\vy}$ is defined by
        \begin{align*}
          \cov\bk{\vy}
          &=
            \begin{bmatrix}
              \EE\bk[Big]{\pn{y_0 - \EE\bk{y_0}}\pn{y_0 - \EE\bk{y_0}}}
              & \EE\bk[Big]{\pn{y_0 - \EE\bk{y_0}}\pn{y_1 - \EE\bk{y_1}}}
              & \cdots
              & \EE\bk[Big]{\pn{y_0 - \EE\bk{y_0}}\pn{y_n - \EE\bk{y_n}}}
              \\[1em]
              \EE\bk[Big]{\pn{y_1 - \EE\bk{y_1}}\pn{y_0 - \EE\bk{y_0}}}
              & \EE\bk[Big]{\pn{y_1 - \EE\bk{y_1}}\pn{y_1 - \EE\bk{y_1}}}
              & \cdots
              & \EE\bk[Big]{\pn{y_1 - \EE\bk{y_1}}\pn{y_n - \EE\bk{y_n}}}
              \\[1em]
              \vdots & \vdots & \ddots & \vdots \\[1em]
              \EE\bk[Big]{\pn{y_n - \EE\bk{y_n}}\pn{y_0 - \EE\bk{y_0}}}
              & \EE\bk[Big]{\pn{y_n - \EE\bk{y_n}}\pn{y_1 - \EE\bk{y_1}}}
              & \cdots
              & \EE\bk[Big]{\pn{y_n - \EE\bk{y_n}}\pn{y_n - \EE\bk{y_n}}}
            \end{bmatrix} \\
          &=
            \begin{bmatrix}
              \cov\pn{y_0,y_0} & \cov\pn{y_0,y_1} & \cdots &
              \cov\pn{y_0, y_n} \\
              \cov\pn{y_1,y_0} & \cov\pn{y_1,y_1} & \cdots &
              \cov\pn{y_1, y_n} \\
              \vdots & \vdots & \ddots & \vdots \\
              \cov\pn{y_n, y_0} & \cov\pn{y_n, y_1} & \cdots &
              \cov\pn{y_n, y_n}
            \end{bmatrix}
        \end{align*}
        it is a property of covariance that for all random variables
        $X$ and $Y$, with scalars $a$ and $b$, $\cov\pn{X+a,Y+b} =
        \cov\pn{X,Y}$. Hence, since each $y_i = (A\vx)_i + b_i$, then
        $\forall i,j \in \set{0,1,\ldots,n}$,
        \begin{align*}
          \cov\bk{y_i,y_j}
          &= \cov\bk{(A\vx)_i + b_i, (A\vx)_j + b_j} \\
          &= \cov\bk{(A\vx)_i, (A\vx)_j}
        \end{align*}
        thus, examining the matrix above, we see $\cov\bk{A\vx + \vb}
        = \cov\bk{A\vx}$. It remains to show $\cov{\bk{A\vx}} =
        A\cov\bk{\vx}A^\top$.
    \end{enumerate}
  \clearpage

% --------------------------- Problem 2 ---------------------------- %

  \section{}
    Given the dataset $\cD = \{(x,y)\} = \{(0,1), (2,3), (3,6), (4,8)\}$
    \begin{enumerate}
      \item Find the least squares estimate $y = {\bm{\theta}}^\T\vx$ by
        hand using Cramer's Rule.
      \item Use the normal equations to find the same solution and
        verify it is the same as part (a).
      \item Plot the data and the optimal linear fit you found.
      \item Find randomly generate 100 points near the line with white
        Gaussian noise and then compute the least squares estimate
        (using a computer). Verify that this new line is close to the
        original and plot the new dataset, the old line, and the new
        line.
    \end{enumerate}

  \hrulefill

  \section*{Solution:}


\end{document}
